{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python373jvsc74a57bd077b122b46b811bf25a94c2daf71b0b6b5d8e93bff1d9be67645d638b69bd8036",
   "display_name": "Python 3.7.3 64-bit ('ds21_capstone': virtualenvwrapper)"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "source": [
    "# Process Data\n",
    "---\n",
    "\n",
    "Clean the CSVs that were provided.\n",
    "\n",
    "1. Clean column names\n",
    "2. Remove unneeded columns (e.g. 6 or 12-months post treatment)\n",
    "3. Remove dropout patients\n",
    "4. Set missing values to NaN"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "## Import Packages & Setup Variables\n",
    "---"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "import os\n",
    "import re\n",
    "\n",
    "from pathlib import Path\n",
    "from collections import OrderedDict\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "from pandas_profiling import ProfileReport"
   ]
  },
  {
   "source": [
    "## Import + Clean Data\n",
    "---"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fix_column_names(df):\n",
    "    \"\"\"\n",
    "    Clean the column names, similar to clean_names in tidyverse janitor\n",
    "      - CamelCase to snake_case\n",
    "      - All characters to lower case\n",
    "      - Remove special characters\n",
    "      - Replace whitespace with underscore (_)\n",
    "    \n",
    "    Returns\n",
    "    ---\n",
    "    pd.DataFrame\n",
    "    \"\"\"\n",
    "    df = df.copy()\n",
    "    \n",
    "    # column names to list\n",
    "    column_list = [*df.columns]\n",
    "\n",
    "    # 3+ capital letters to lowercase (e.g. API)\n",
    "    column_list = \\\n",
    "      [re.sub(r\"[A-Z]{3,}\", lambda m: m.group(0).lower(), x) for x in column_list]\n",
    "\n",
    "    # # convert to snake_case\n",
    "    column_list = [re.sub(r'(?<!^)(?=[A-Z])', '_', x) for x in column_list]\n",
    "    \n",
    "    # # remove special characters\n",
    "    column_list = [re.sub(r\"[^\\w\\s]\", '', x) for x in column_list]\n",
    "\n",
    "    # # whitespace to underscore\n",
    "    column_list = [re.sub(r\"\\s+\", '_', x) for x in column_list]\n",
    "\n",
    "    # # Fix double underscores\n",
    "    column_list = [re.sub(r\"(_{2,})\", \"_\", x) for x in column_list]\n",
    "    \n",
    "    # assign to dataframe\n",
    "    df.columns = column_list\n",
    "\n",
    "    # lower case\n",
    "    df = df.rename(str.lower, axis='columns')\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"../data/chronic_pain_reduced.csv\")\n",
    "df = fix_column_names(df)"
   ]
  },
  {
   "source": [
    "## Drop Columns + Dropout Patients!\n",
    "---\n",
    "* Drop columns that we will not use (e.g. post-6, post-12 month data)\n",
    "* Remove patients that were dropouts (10 total)\n",
    "* Set Missing values to NA"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop records containing values\n",
    "# Using an OrderedDict in case there is a specific reason to remove something first\n",
    "record_drop = OrderedDict({\n",
    "    'status': ['Dropout Post-Randomization']\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Replace missing value placeholders with np.nan()\n",
    "missing_values = {\n",
    "    'api_tot_recode0_4_post': 9999,\n",
    "    'csigimn2019_post': ' ',\n",
    "    'cprinterf8_post': 9999,\n",
    "    'cprint8_tscore_post': 9999,\n",
    "    'pbq3_txfap': ' ',\n",
    "    'pbq6_txfap': ' ',\n",
    "    'pbq13_txfap': ' ',\n",
    "    'pbq14_txfap': ' ',\n",
    "    'prsleep8_txfap': ' '\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop columns\n",
    "cols_to_drop = [\n",
    "    'redcap_survey_identifier',                 # unique ID for each survey participant\n",
    "    'status',                                   # Enrolled. Already dropped the dropouts\n",
    "    'api_tot_recode0_4_mid',\n",
    "    'api_tot_recode0_4_6mo',\n",
    "    'api_tot_recode0_4_12mo',\n",
    "    'csigimn2019_mid',\n",
    "    'csigimn2019_6mo',\n",
    "    'csigimn2019_12mo',\n",
    "    'cprinterf8_mid',\n",
    "    'cprint8_tscore_mid',\n",
    "    'cprinterf8_6mo',\n",
    "    'cprint8_tscore_6mo',\n",
    "    'cprinterf8_12mo',\n",
    "    'cprint8_tscore_12mo',\n",
    "    'dose',\n",
    "    'teen_module'\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Drop values from DF\n",
    "for key, value in record_drop.items():\n",
    "    df = df[~df[key].isin(value)]\n",
    "\n",
    "# Drop them columns\n",
    "df = df.drop(cols_to_drop, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Missing values to NaN\n",
    "for key, value in missing_values.items():\n",
    "    df.loc[df[key] == value, key] = np.nan\n",
    "\n",
    "# Quick remedy for now --  Replace all empty strings with nan\n",
    "df = df.replace(' ', np.nan, regex=True)\n",
    "df = df.replace('', np.nan, regex=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop those where we have a missing outcome variable - not much we can do there\n",
    "# Removes 43 records (16%)\n",
    "df = df.dropna(subset = [\n",
    "    'api_tot_recode0_4_post', 'csigimn2019_post', 'cprinterf8_post', 'cprint8_tscore_post',\n",
    "    'api_tot_recode0_4_base', 'csigimn2019_base', 'cprinterf8_base', 'cprint8_tscore_base'\n",
    "])"
   ]
  },
  {
   "source": [
    "### Write CSV #1 -- Includes Missing Data\n",
    "\n",
    "Can be used to test imputing missing values later on"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv(\"../data/cleaned/not_scaled_with_missing.csv\", index=False, na_rep='NULL')"
   ]
  },
  {
   "source": [
    "### Write CSV #2 -- Excludes Missing Data"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop NA for remaining data -- Need to think of a strategy\n",
    "# Go from 268 to 225 records (43 records 16% loss)\n",
    "# 19 from Study Group 2, 14 from Study Group 1\n",
    "\n",
    "# IF WE DO THIS TO ALL DATA COLUMNS WE END UP WITH ONLY 184 RECORDS (-84, 31%)\n",
    "df_no_missing = df.dropna(axis=0).copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_no_missing.to_csv(\"../data/cleaned/not_scaled_with_missing.csv\", index=False, na_rep='NULL')"
   ]
  },
  {
   "source": [
    "## Rescale Variables\n",
    "---\n",
    "\n",
    "We want all of our features to be aligned with how they scale (low to high). In the code below we assume any feature not listed is already scaled low-high.\n",
    "\n",
    "Repeat the same code block twice -- one that includes missing data, one that does not have missing data"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reverse the scaling based on min/max values provided (don't want to infer directly from data)\n",
    "# Assuming all values are integers, for now\n",
    "# Notes here: https://docs.google.com/spreadsheets/d/1wSv6wUGWKldj-Rhy_BoqFUwiF6j74xf96LVe9aiPM1Q/edit#gid=0\n",
    "rescale_columns = {\n",
    "    \"pbq1_txfap\": [0, 4],\n",
    "    \"pbq3_txfap\": [0, 4],\n",
    "    \"pbq6_txfap\": [0, 4],\n",
    "    \"pbq7_txfap\": [0, 4],\n",
    "    \"pbq8_txfap\": [0, 4],\n",
    "    \"pbq11_txfap\": [0, 4],\n",
    "    \"pbq13_txfap\": [0, 4],\n",
    "    \"pbq14_txfap\": [0, 4],\n",
    "    \"pbq15_txfap\": [0, 4],\n",
    "    \"pbq16_txfap\": [0, 4],\n",
    "    \"pbq17_txfap\": [0, 4],\n",
    "    \"pbq18_txfap\": [0, 4],\n",
    "    \"prsleep2_txfap\": [0, 4],\n",
    "    \"prsleep3_txfap\": [0, 4],\n",
    "    \"prsleep8_txfap\": [1, 5],\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "[4, 3, 2, 1, 0]\nDidn't update rescaler --- pbq3_txfap\nDidn't update rescaler --- pbq6_txfap\nDidn't update rescaler --- pbq7_txfap\nDidn't update rescaler --- pbq8_txfap\nDidn't update rescaler --- pbq11_txfap\nDidn't update rescaler --- pbq13_txfap\nDidn't update rescaler --- pbq14_txfap\nDidn't update rescaler --- pbq15_txfap\nDidn't update rescaler --- pbq16_txfap\nDidn't update rescaler --- pbq17_txfap\nDidn't update rescaler --- pbq18_txfap\nDidn't update rescaler --- prsleep2_txfap\nDidn't update rescaler --- prsleep3_txfap\n[0, 5, 4, 3, 2, 1]\n"
     ]
    }
   ],
   "source": [
    "# DF - WITH MISSING DATA\n",
    "# Loop through each item in the column list\n",
    "previous_scale = ''\n",
    "\n",
    "for col, question_scale in rescale_columns.items():\n",
    "    \n",
    "    # Error handling\n",
    "    if col not in df.columns:\n",
    "        print('NEED TO RAISE ERROR!')\n",
    "    \n",
    "    # Skip if the same values as previous column\n",
    "    if previous_scale != question_scale:\n",
    "\n",
    "        # Create reversed question scale (inclusive)\n",
    "        quest_scale_reverse = [*range(question_scale[0], question_scale[1] + 1)][::-1]\n",
    "\n",
    "        # prepend 0s if starting at non-zero\n",
    "        if question_scale[0] != 0:\n",
    "            quest_scale_reverse = [0] * question_scale[0] + quest_scale_reverse\n",
    "\n",
    "        print(quest_scale_reverse)\n",
    "\n",
    "    else:\n",
    "        print(f'Didn\\'t update rescaler --- {col}')\n",
    "    \n",
    "    # rescale all data points in that \n",
    "    df[col] = df[col].map(lambda x: quest_scale_reverse[int(x)], na_action='ignore')\n",
    "    \n",
    "    # Last thing to do\n",
    "    previous_scale = question_scale\n",
    "\n",
    "df.to_csv(\"../data/cleaned/scaled_with_missing.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "[4, 3, 2, 1, 0]\nDidn't update rescaler --- pbq3_txfap\nDidn't update rescaler --- pbq6_txfap\nDidn't update rescaler --- pbq7_txfap\nDidn't update rescaler --- pbq8_txfap\nDidn't update rescaler --- pbq11_txfap\nDidn't update rescaler --- pbq13_txfap\nDidn't update rescaler --- pbq14_txfap\nDidn't update rescaler --- pbq15_txfap\nDidn't update rescaler --- pbq16_txfap\nDidn't update rescaler --- pbq17_txfap\nDidn't update rescaler --- pbq18_txfap\nDidn't update rescaler --- prsleep2_txfap\nDidn't update rescaler --- prsleep3_txfap\n[0, 5, 4, 3, 2, 1]\n"
     ]
    }
   ],
   "source": [
    "# DF - NO MISSING DATA\n",
    "# Loop through each item in the column list\n",
    "previous_scale = ''\n",
    "\n",
    "for col, question_scale in rescale_columns.items():\n",
    "    \n",
    "    # Error handling\n",
    "    if col not in df_no_missing.columns:\n",
    "        print('NEED TO RAISE ERROR!')\n",
    "    \n",
    "    # Skip if the same values as previous column\n",
    "    if previous_scale != question_scale:\n",
    "\n",
    "        # Create reversed question scale (inclusive)\n",
    "        quest_scale_reverse = [*range(question_scale[0], question_scale[1] + 1)][::-1]\n",
    "\n",
    "        # prepend 0s if starting at non-zero\n",
    "        if question_scale[0] != 0:\n",
    "            quest_scale_reverse = [0] * question_scale[0] + quest_scale_reverse\n",
    "\n",
    "        print(quest_scale_reverse)\n",
    "\n",
    "    else:\n",
    "        print(f'Didn\\'t update rescaler --- {col}')\n",
    "    \n",
    "    # rescale all data points in that \n",
    "    df_no_missing[col] = df_no_missing[col].map(lambda x: quest_scale_reverse[int(x)], na_action='ignore')\n",
    "    \n",
    "    # Last thing to do\n",
    "    previous_scale = question_scale\n",
    "\n",
    "df_no_missing.to_csv(\"../data/cleaned/scaled_no_missing.csv\", index=False)"
   ]
  },
  {
   "source": [
    "## Pandas Profiling\n",
    "---"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "profile = ProfileReport(df_no_missing, title='Pandas Profiling Report')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "Summarize dataset: 100%|██████████| 144/144 [01:57<00:00,  1.23it/s, Completed]\n",
      "Generate report structure: 100%|██████████| 1/1 [00:45<00:00, 45.00s/it]\n",
      "Render HTML: 100%|██████████| 1/1 [00:07<00:00,  7.89s/it]\n",
      "Export report to file: 100%|██████████| 1/1 [00:00<00:00, 23.52it/s]\n"
     ]
    }
   ],
   "source": [
    "profile.to_file(\"../pandas_profiling_report.html\")"
   ]
  }
 ]
}